{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import numba\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../model/'\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @numba.jit\n",
    "def stress_pos(x):\n",
    "    try:\n",
    "        res = np.zeros(len(x)-1)\n",
    "        stress_idx = x.find('+')\n",
    "        res[stress_idx] = 1\n",
    "        return res\n",
    "    except IndexError:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_accents(x):\n",
    "    chars = list(x.encode('utf-8').replace(b'\\xcc\\x81', b'+').decode('utf-8'))\n",
    "\n",
    "    final_chars = []\n",
    "    for i, c in enumerate(chars):\n",
    "        if c == '+':\n",
    "            try:\n",
    "                tmp = final_chars[i - 1]\n",
    "\n",
    "                final_chars.pop()\n",
    "                final_chars.append('+')\n",
    "                final_chars.append(tmp)\n",
    "            except IndexError:\n",
    "                final_chars.append(c)\n",
    "        else:\n",
    "            final_chars.append(c)\n",
    "\n",
    "    return ''.join(final_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for file in tqdm(glob.glob(DATA_PATH + \"*.txt\")):\n",
    "    with open(file) as fil:\n",
    "        lines = fil.readlines()\n",
    "        for line in lines:\n",
    "            try:\n",
    "                word2 = line.strip()\n",
    "                word2 = replace_accents(word2)\n",
    "                word1 = word2.replace(\"+\",\"\")\n",
    "                word1, word2 = word1.strip().replace('i', 'і'), word2.strip().replace('i', 'і')\n",
    "                data.append((word1, word2))\n",
    "            except ValueError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)\n",
    "data.columns = ['word', 'word_with_stress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_list = [\n",
    "    '\\t', '$', '%', '-', '\\\\', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', \n",
    "    'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', \n",
    "    'v', 'w', 'x', 'y', 'z', '~', '\\xa0', '\\xad', '·', 'ђ', 'ј', 'љ', 'њ', 'ћ', \n",
    "    'ў', 'џ', '–', '‘', '’', '“', '”', '•', '№',\n",
    "    'ъ', 'ы', 'э'\n",
    "]\n",
    "\n",
    "for item in tqdm(exclude_list):\n",
    "    data = data.loc[~data.word.str.contains(item, regex=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data['word_list'] = data.word.map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data['word_stress_pos'] = data.word_with_stress.map(stress_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data.word_stress_pos.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = np.max(data.word.str.len())\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(array):\n",
    "    for item in array:\n",
    "        if isinstance(item, list):\n",
    "            yield from flatten(item)\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "\n",
    "class SequenceTokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.oov_token ='<UNK>'\n",
    "        self.oov_token_index = 0\n",
    "        \n",
    "    def fit(self, sequence):\n",
    "        self.index2word = dict(enumerate([self.oov_token] + sorted(set(flatten(sequence))), 1))\n",
    "        self.word2index = {v:k for k,v in self.index2word.items()}\n",
    "        self.oov_token_index = self.word2index.get(self.oov_token)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        res = []\n",
    "        for line in X:\n",
    "            res.append([self.word2index.get(item, self.oov_token_index) for item in line])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SequenceTokenizer()\n",
    "tokenizer.fit(data.word_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.transform(data.word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(lst, max_seq=max_sequence_len):\n",
    "    if isinstance(lst[0], list):\n",
    "        return np.array([i + [0]*(max_seq-len(i)) for i in lst])\n",
    "    else:\n",
    "        lst + [0]*(max_seq-len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# input_seq = zip(*itertools.zip_longest(*X, fillvalue=0))\n",
    "# input_seq = list(map(list, input_seq))\n",
    "# input_seq = np.array(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_seq = pad_sequence(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_seq.shape)\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.word_stress_pos.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seq = zip(*itertools.zip_longest(*y, fillvalue=0))\n",
    "output_seq = list(map(list, output_seq))\n",
    "output_seq = np.array(output_seq).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_seq.shape)\n",
    "output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(input_seq_train, input_seq_val, \n",
    " output_seq_train, output_seq_val) = train_test_split(input_seq, \n",
    "                                                      output_seq, \n",
    "                                                      test_size=0.5, \n",
    "                                                      random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_train = torch.tensor(input_seq_train, dtype=torch.long).cuda()\n",
    "input_seq_val = torch.tensor(input_seq_val, dtype=torch.long).cuda()\n",
    "output_seq_train = torch.tensor(output_seq_train, dtype=torch.float).cuda()\n",
    "output_seq_val = torch.tensor(output_seq_val, dtype=torch.float).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data,target = self.dataset[index]\n",
    "        return data, target, index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.embeddings.embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=3,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim * 4 , 64)\n",
    "        self.batch_norm = nn.BatchNorm1d(self.hidden_dim * 4, affine=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.out = nn.Linear(64, target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_embeddings = self.embeddings(x)\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embeddings)\n",
    "        avg_pool = torch.mean(h_lstm, 1)\n",
    "        max_pool, _ = torch.max(h_lstm, 1)\n",
    "        x = torch.cat((avg_pool, max_pool), 1)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        y = nn.functional.softmax(x, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_model(embedding_dim=64, \n",
    "                   hidden_dim=64, \n",
    "                   vocab_size=len(tokenizer.word2index) + 1, \n",
    "                   target_size=max_sequence_len)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256 * 2\n",
    "\n",
    "train = MyDataset(torch.utils.data.TensorDataset(input_seq_train, output_seq_train))\n",
    "valid = MyDataset(torch.utils.data.TensorDataset(input_seq_val, output_seq_val))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     (x_sample, y_sample, index) = train[:2]\n",
    "#     preds = model(x_sample.reshape(1, -1))\n",
    "#     print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "# kf.get_n_splits(X=input_seq, y=output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(X=input_seq, y=output_seq):\n",
    "    print(train_index.shape, test_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "history = {'train': {}, 'val': {}}\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    avg_loss, total_loss, avg_acc, total_acc = 0., 0., 0., 0.\n",
    "    for i, (x_batch, y_batch, index) in enumerate(train_loader):\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_function(y_pred, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        equal = torch.eq(torch.argmax(y_pred, axis=1), torch.argmax(y_batch, axis=1))\n",
    "        batch_acc = int(equal.sum(-1)) / y_batch.shape[0]\n",
    "        batch_loss = loss.item()\n",
    "        \n",
    "        total_acc += batch_acc\n",
    "        total_loss += batch_loss\n",
    "        print(f\"\\rEpoch [{epoch}/{n_epochs}] \"\n",
    "              f\" progress = {round(i/len(train_loader)*100)}% \"\n",
    "              f\"\\t loss={total_loss / (i + 1):.4f} \"\n",
    "              f\"\\t acc={total_acc / (i + 1) * 100:.2f}% \", end='')\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_acc = total_acc / len(train_loader)\n",
    "    history['train']['loss'] = history.get('train', {}).get('loss', []) + [avg_loss]\n",
    "    history['train']['accuracy'] = history.get('train', {}).get('accuracy', []) + [avg_acc]\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    avg_val_loss, total_val_loss, avg_val_acc, total_val_acc = 0., 0., 0., 0.\n",
    "    for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "        val_loss = loss_function(y_pred, y_batch)\n",
    "        \n",
    "        equal = torch.eq(torch.argmax(y_pred, axis=1), torch.argmax(y_batch, axis=1))\n",
    "        batch_val_acc = int(equal.sum(-1)) / y_batch.shape[0]\n",
    "        batch_val_loss = val_loss.item()\n",
    "        \n",
    "        total_val_acc += batch_val_acc\n",
    "        total_val_loss += batch_val_loss\n",
    "    avg_val_loss = total_val_loss / len(valid_loader)\n",
    "    avg_val_acc = total_val_acc / len(valid_loader)\n",
    "    history['val']['loss'] = history.get('val', {}).get('loss', []) + [avg_val_loss]\n",
    "    history['val']['accuracy'] = history.get('val', {}).get('accuracy', []) + [avg_val_acc]\n",
    "    \n",
    "    elapsed_time = time.time() - start_time \n",
    "    print(f\"\\nEpoch [{epoch}/{n_epochs}]  results:\"\n",
    "          f\"\\t\\t loss={avg_loss:.4f}\"\n",
    "          f\"\\t acc={avg_acc * 100:.2f}%\"\n",
    "          f\"\\t val_loss={avg_val_loss:.4f}\"\n",
    "          f\"\\t val_acc={avg_val_acc * 100:.2f}%\"\n",
    "          f\"\\t time={elapsed_time:.2f}s\")\n",
    "    print(\"-\"*78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.plot(history['train']['loss'])\n",
    "plt.plot(history['val']['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('val')\n",
    "plt.xlabel('epoch')\n",
    "plt.xticks(np.arange(len(history['train']['loss'])), np.arange(1, len(history['train']['loss']) + 1))\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.plot(history['train']['accuracy'])\n",
    "plt.plot(history['val']['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.xticks(np.arange(len(history['train']['accuracy'])), np.arange(1, len(history['train']['accuracy']) + 1))\n",
    "plt.legend(['train', 'val'], loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(words, mode='stress'):\n",
    "    tokens = pad_sequence(tokenizer.transform(words))\n",
    "    sequences = torch.tensor(tokens, dtype=torch.long).cuda()\n",
    "    preds = model(sequences)\n",
    "    indeces = torch.argmax(preds, axis=1)\n",
    "    if mode == 'stress':\n",
    "        return [word[:index+1] + chr(769) + word[index+1:] for word, index in zip(words, indeces)]\n",
    "    elif mode == 'asterisk':\n",
    "        return [word[:index+1] + \"*\" + word[index+1:] for word, index in zip(words, indeces)]\n",
    "    else:\n",
    "        raise ValueError(f\"Wrong `mode`={mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"словотворення\", \"архаїчний\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"accentor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
